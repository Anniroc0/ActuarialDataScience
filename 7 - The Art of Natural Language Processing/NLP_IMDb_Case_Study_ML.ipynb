{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *The Art of Natural Language Processing: Machine Learning for the Case Study*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Authors: Andrea Ferrario, Mara NÃ¤gelin**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Date: February 2020**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to run the machine learning modeling in the Classical and Modern Approaches, as described in the tutorial `The Art of Natural Language Processing: Classical, Modern and Contemporary Approaches to Text Document Classification'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Getting started with Python and Jupyter Notebook](#started)\n",
    "2. [Import data](#import)\n",
    "3. [Duplicated reviews](#duplicated)\n",
    "4. [Data preprocessing](#preprocessing)\n",
    "5. [POS-tagging](#POS)\n",
    "6. [Pre-trained word embeddings](#emb)\n",
    "7. [Data analytics](#analytics)  \n",
    "    7.1. [A quick check of data structure](#check)  \n",
    "    7.2. [Basic linguistic analysis of movie reviews](#basic)\n",
    "8. [Machine learning](#ML)  \n",
    "    8.1. [Adaptive boosting (ADA)](#ADA)  \n",
    "    .......8.1.1. [Bag-of-words](#ADA_BOW)  \n",
    "    .......8.1.2. [Bag-of-POS](#ADA_BOP)  \n",
    "    .......8.1.3. [Embeddings](#ADA_E)  \n",
    "    8.2. [Random forests (RF)](#RF)  \n",
    "    .......8.2.1. [Bag-of-words](#RF_BOW)  \n",
    "    .......8.2.2. [Bag-of-POS](#RF_BOP)  \n",
    "    .......8.2.3. [Embeddings](#RF_E)  \n",
    "    8.3. [Extreme gradient boosting (XGB)](#XGB)  \n",
    "    .......8.3.1. [Bag-of-words](#XGB_BOW)  \n",
    "    .......8.3.2. [Bag-of-POS](#XGB_BOP)  \n",
    "    .......8.3.3. [Embeddings](#XGB_E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Getting started with Python and Jupyter Notebook<a name=\"started\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, Jupyter Notebook and Python settings are initialized. For code in Python, the [PEP8 standard](https://www.python.org/dev/peps/pep-0008/) (\"PEP = Python Enhancement Proposal\") is enforced with minor variations to improve readibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Notebook settings\n",
    "###################\n",
    "\n",
    "# resetting variables\n",
    "get_ipython().magic('reset -sf') \n",
    "\n",
    "# formatting: cell width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import data<a name=\"import\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the import function, as in Chapter 8 of Raschka's book (see the tutorial)\n",
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "basepath = '...' # insert basepath, where original data are stored\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file), \n",
    "                      'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], \n",
    "                           ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Duplicated reviews<a name=\"duplicated\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "duplicates = df[df.duplicated()]  #equivalent to keep = first. Duplicated rows, except the first entry, are marked as 'True'\n",
    "print(len(duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a check on the duplicated review\n",
    "duplicates.review   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates: \n",
    "df = df.drop_duplicates()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check\n",
    "df[df.duplicated(subset='review')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Data preprocessing<a name=\"preprocessing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of 'raw' review: we have all sort of HTML markup\n",
    "df.loc[500, 'review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing by Raschka, Chpater 8 (see tutorial)\n",
    "# we remove all markups, substitute non-alphanumeric characters (including \n",
    "# underscore) with whitespaces, and remove the nose from emoticons\n",
    "import re\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
    "            ' '.join(emoticons).replace('-', ''))\n",
    "    return text\n",
    "\n",
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking again the same review\n",
    "df.loc[500, 'review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed data as csv \n",
    "path = '...'  # insert path\n",
    "df.to_csv(path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. POS - tagging<a name=\"POS\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we apply POS-tagging on (deduplicated and) pre-processed data - let us import them\n",
    "path = '...' # insert path\n",
    "df = pd.read_csv(path, encoding='utf-8')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we import the NLTK resources\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "# introduction of POS tagger per NLTK token\n",
    "def pos_tags(text):\n",
    "    text_processed = word_tokenize(text)\n",
    "    return \"-\".join( tag for (word, tag) in nltk.pos_tag(text_processed))\n",
    "\n",
    "# applying POS tagger to data \n",
    "############################################\n",
    "df['text_pos']=df.apply(lambda x: pos_tags(x['review']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save POS-tagged data as csv \n",
    "path = '...' # insert path\n",
    "df.to_csv(path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Pre-trained word embeddings<a name=\"emb\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we apply embeddings on de-duplicated and pre-processed data - let us import them\n",
    "path = '...' # insert path\n",
    "df = pd.read_csv(path, encoding='utf-8')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pre-trained word embedding model\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we stack (like a numpy vertical stack) the 300 variables obtained from averaging the embedding of each df.review entry\n",
    "import numpy as np\n",
    "emb = np.vstack(df.review.apply(lambda x: nlp(x).vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings into a dataframe\n",
    "emb = pd.DataFrame(emb, columns = np.array([str(x) for x in range(0, 299 + 1)]) )\n",
    "print(emb.shape)\n",
    "print(emb.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join embeddings with dataframe\n",
    "df_embed = pd.concat([df, emb], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the resulting dataframe\n",
    "df_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save word embedding data as csv \n",
    "path = '...' # insert path\n",
    "df_embed.to_csv(path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Data analytics<a name=\"analytics\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reproduce main data analytics results in Section 6.3 of the tutorial. We use the preprocessed and deduplicated data, for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. A quick check of data structure<a name=\"check\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data\n",
    "import pandas as pd\n",
    "\n",
    "path = '...' # insert path for deduplicated and preprocessed data\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imported data structure\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns in data\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imported data: first 10 entries\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts of rviews per sentiment value\n",
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Basic linguistic analysis of movie reviews<a name=\"basic\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show distribution of review lenghts \n",
    "# we strip leading and trailing whitespaces and tokenize by whitespace\n",
    "df['word_count'] = df['review'].apply(lambda x: len(x.strip().split(\" \")))\n",
    "df[['review','sentiment', 'word_count']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary statistics of word counts\n",
    "print(df['word_count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show histograms of word counts divided per sentiment\n",
    "from matplotlib import pyplot\n",
    "\n",
    "x = df[df['sentiment']==0].word_count\n",
    "y = df[df['sentiment']==1].word_count\n",
    "\n",
    "pyplot.hist(x, bins=50, alpha=0.5, label='negative sentiment reviews')\n",
    "pyplot.hist(y, bins=50, alpha=0.5, label='positive sentiment reviews')\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary of distributions of word counts\n",
    "print(x.describe())\n",
    "print(y.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some checks (e.g. word_counts=6 or 1550 or 2498 )\n",
    "df[df['word_count']==6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average word length (again, we tokenize by whitespaces)\n",
    "def avg_word(sentence):\n",
    "  words = sentence.split()\n",
    "  return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "df['avg_word'] = df['review'].apply(lambda x: avg_word(x.strip()))\n",
    "df[['review','word_count', 'sentiment', 'avg_word']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributions of word lengths conditional per sentiment\n",
    "x = df[df['sentiment']==0].avg_word\n",
    "y = df[df['sentiment']==1].avg_word\n",
    "print(x.describe())\n",
    "print()\n",
    "print(y.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some checks (e.g. avg_word>=11)\n",
    "df[df['avg_word']>=11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words statistics - stopword from NLTK\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "df['stopwords'] = df['review'].apply(lambda x: len([x for x in x.strip().split() if x in stop]))\n",
    "df[['review','word_count', 'sentiment', 'avg_word', 'stopwords']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributions of stop words conditional per sentiment\n",
    "x = df[df['sentiment']==0].stopwords\n",
    "y = df[df['sentiment']==1].stopwords\n",
    "print(x.describe())\n",
    "print()\n",
    "print(y.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some checks (e.g. stopwords==0)\n",
    "df[df['stopwords']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Machine Learning<a name=\"ML\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We replicate the machine learning pipelines from the tutorial, Section 6.4 (Classical and Modern Approaches)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**: as mentioned in the tutorial, the following cross-validation routines are computationally intensive. We recommend to sub-sample data and/or use HPC infrastructure (specifying the parameter njobs in GridSearch() accordingly). Test runs can be launched on reduced hyperparameter grids, as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Adaptive boosting (ADA)<a name=\"ADA\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the adaptive boosting (ADA) algorithm on top of NLP pipelines (bag-of models and pre-trained word embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.1. Bag-of-words<a name=\"ADA_BOW\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import \n",
    "import pandas as pd\n",
    "\n",
    "path = '...'           # insert path to deduplicated and preprocessed data\n",
    "df = pd.read_csv(path)     \n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# introducing the stopwords\n",
    "###########################################\n",
    "\n",
    "# stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = list(set(stopwords.words('english')))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split\n",
    "##########################################\n",
    "\n",
    "X_train = df.head(39666).review\n",
    "y_train = df.head(39666).sentiment\n",
    "\n",
    "X_test = df.tail(9916).review\n",
    "y_test = df.tail(9916).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = {'vect__ngram_range': [(1, 1)],           # choose (1, 2) to compute 2-grams\n",
    "              'vect__stop_words': [stopwords, None],\n",
    "              'vect__max_df': [1.0, 0.1, 0.3, 0.5],\n",
    "              'vect__max_features': [None, 1000],                                          \n",
    "              'clf__n_estimators': [100, 200, 300, 400],\n",
    "              'clf__learning_rate': [0.001, 0.01, 0.1, 1.0]\n",
    "              }\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', AdaBoostClassifier(base_estimator=tree))]\n",
    "                    )\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv, \n",
    "                           n_jobs=)               # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.2. Bag-of-POS<a name=\"ADA_BOP\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import\n",
    "import pandas as pd\n",
    "\n",
    "path = '...'  # insert path to data with POS-tags\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split\n",
    "##########################################\n",
    "\n",
    "X_train = df.head(39666).text_pos\n",
    "y_train = df.head(39666).sentiment\n",
    "\n",
    "X_test = df.tail(9916).text_pos\n",
    "y_test = df.tail(9916).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = {'vect__ngram_range': [(1, 1)],               # we consider only 1-gram POS (for 2-grams: (1,2))\n",
    "              'clf__n_estimators': [100, 200, 300, 400],\n",
    "              'clf__learning_rate': [0.001, 0.01, 0.1, 1.0]}\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', AdaBoostClassifier(base_estimator=tree))]\n",
    "                    )\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv, \n",
    "                           n_jobs=)                        # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.3. Embeddings<a name=\"ADA_E\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import\n",
    "import pandas as pd\n",
    "\n",
    "path = '...'  # insert path to data with pre-trained word embeddings\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split - drop the variables\n",
    "############################################################################\n",
    "\n",
    "X_train = df.drop(columns=['review', 'sentiment']).head(39666)    # we use only the 300 embeddings\n",
    "y_train = df.head(39666).sentiment\n",
    "\n",
    "X_test = df.drop(columns=['review', 'sentiment']).tail(9916)      # we use only the 300 embeddings\n",
    "y_test = df.tail(9916).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'clf__n_estimators': [100, 200, 300, 400],\n",
    "              'clf__learning_rate': [0.001, 0.01, 0.1, 1.0]}\n",
    "\n",
    "# extended parameter grid (Table 6, Section 6.4.5 in the tutorial)\n",
    "# param_grid = {'clf__n_estimators': [100, 200, 300, 400, 500, 700, 900, 1000],\n",
    "#              'clf__learning_rate': [0.001, 0.01, 0.1, 1.0]}\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "pipe = Pipeline([('clf', AdaBoostClassifier(base_estimator=tree))])\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(pipe, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=cv, \n",
    "                           n_jobs=)                        # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. Random Forests (RF)<a name=\"RF\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the random forests (RF) algorithm on top of NLP pipelines (bag-of models and pre-trained word embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.1. Bag-of-words<a name=\"RF_BOW\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import\n",
    "import pandas as pd\n",
    "\n",
    "path = '...' # insert path to deduplicated and preprocessed data\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# introducing the stopwords\n",
    "###########################################\n",
    "\n",
    "# stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = list(set(stopwords.words('english')))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split\n",
    "##########################################\n",
    "\n",
    "X_train = df.head(39666).review\n",
    "y_train = df.head(39666).sentiment\n",
    "\n",
    "X_test = df.tail(9916).review\n",
    "y_test = df.tail(9916).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = {'vect__ngram_range': [(1, 1)],    # for 2-grams:(1, 2)\n",
    "              'vect__stop_words': [stopwords, None],\n",
    "              'vect__max_df': [1.0, 0.1, 0.3, 0.5],\n",
    "              'vect__max_features': [None, 1000],                                           \n",
    "              'clf__n_estimators': [100, 200, 300, 400],\n",
    "              'clf__max_depth': [1, 5, 10]\n",
    "              }\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', RandomForestClassifier())]\n",
    "                    )\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv, \n",
    "                           n_jobs=)       # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2. Bag-of-POS<a name=\"RF_BOP\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import\n",
    "import pandas as pd\n",
    "\n",
    "path = '...'  # insert path to data with POS-tags\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split\n",
    "##########################################\n",
    "\n",
    "X_train = df.head(39666).text_pos\n",
    "y_train = df.head(39666).sentiment\n",
    "\n",
    "X_test = df.tail(9916).text_pos\n",
    "y_test = df.tail(9916).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = {'vect__ngram_range': [(1, 1)],               # we consider only 1-gram POS (for 2-grams: (1,2))\n",
    "              'clf__n_estimators': [100, 200, 300, 400],\n",
    "              'clf__max_depth': [1, 5, 10]\n",
    "              }\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', RandomForestClassifier())]\n",
    "                    )\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv, \n",
    "                           n_jobs=)              # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.3. Embeddings<a name=\"RF_E\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import \n",
    "import pandas as pd\n",
    "\n",
    "path = '...'  # insert path to data with pre-trained word embeddings\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split - drop the variables\n",
    "############################################################################\n",
    "\n",
    "X_train = df.drop(columns=['review', 'sentiment']).head(39666)    # we use only the 300 embeddings\n",
    "y_train = df.head(39666).sentiment\n",
    "\n",
    "X_test = df.drop(columns=['review', 'sentiment']).tail(9916)     # we use only the 300 embeddings\n",
    "y_test = df.tail(9916).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "              'clf__n_estimators': [100, 200, 300, 400],\n",
    "              'clf__max_depth': [1, 5, 10]\n",
    "             }\n",
    "\n",
    "# extended parameter grid (Table 6, Section 6.4.5 in the tutorial)\n",
    "# param_grid = {'clf__n_estimators': [100, 200, 300, 400, 500, 600, 800, 1000],\n",
    "#              'clf__max_depth': [1, 5, 10, 20]}\n",
    "\n",
    "pipe = Pipeline([('clf', RandomForestClassifier())\n",
    "               ])\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(pipe, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv, \n",
    "                           n_jobs=)               # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. Extreme gradient boosting (XGB)<a name=\"XGB\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the extreme gradient boosting (XGB) algorithm on top of NLP pipelines (bag-of models and pre-trained word embeddings). We can use the cell below to install xgboost, if other imports failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing xgboost REMARK: run this cell only if other imports failed. Delete it in case xgboost has been already imported\n",
    "import pip\n",
    "pip.main(['install', 'xgboost'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.1. Bag-of-words<a name=\"XGB_BOW\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import\n",
    "import pandas as pd\n",
    "\n",
    "path = '...'  # insert path to preprocessed and deduplicated data\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# introducing the stopwords\n",
    "###########################################\n",
    "\n",
    "# stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = list(set(stopwords.words('english')))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split\n",
    "##########################################\n",
    "\n",
    "X_train = df.head(39666).review\n",
    "y_train = df.head(39666).sentiment\n",
    "\n",
    "X_test = df.tail(9916).review\n",
    "y_test = df.tail(9916).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = {'vect__ngram_range': [(1, 1)]    # for 2-grams: (1,2)\n",
    "              'vect__stop_words': [stopwords, None],\n",
    "              'vect__max_df': [1.0, 0.1, 0.3, 0.5],\n",
    "              'vect__max_features': [None, 1000],                                           \n",
    "              'clf__n_estimators': [100, 300, 500, 1000],\n",
    "              'clf__learning_rate': [0.001, 0.01, 0.1, 1.0],\n",
    "              'clf__max_depth': [1, 10, 20]\n",
    "              }\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', XGBClassifier())]\n",
    "                    )\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv, \n",
    "                           n_jobs=)                 # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.2. Bag-of-POS<a name=\"XGB_BOP\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import\n",
    "import pandas as pd\n",
    "\n",
    "path = '...'  # insert data with POS-tags\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split\n",
    "##########################################\n",
    "\n",
    "X_train = df.head(39666).review\n",
    "y_train = df.head(39666).sentiment\n",
    "\n",
    "X_test = df.tail(9916).review\n",
    "y_test = df.tail(9916).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = {'vect__ngram_range': [(1, 1)],               # we consider only 1-gram POS (for 2-grams: (1,2))\n",
    "              'clf__n_estimators': [100, 300, 500, 1000],\n",
    "              'clf__learning_rate': [0.001, 0.01, 0.1, 1.0],\n",
    "              'clf__max_depth': [1, 10, 20]\n",
    "              }\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', XGBClassifier())]\n",
    "                    )\n",
    "\n",
    "\n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv, \n",
    "                           n_jobs=)              # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3.3. Embeddings<a name=\"XGB_E\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Python packages\n",
    "#########################\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# data preparation\n",
    "###########################################\n",
    "\n",
    "# data import\n",
    "import pandas as pd\n",
    "\n",
    "path = '...'  # insert data with pre-trained word embeddings\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# shuffling data\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# train vs. test: we already shuffled data -80/20 split - drop the variables\n",
    "############################################################################\n",
    "\n",
    "X_train = df.drop(columns=['review', 'sentiment']).head(39666)    # we use only the 300 embeddings\n",
    "y_train = df.head(39666).sentiment\n",
    "\n",
    "X_test = df.drop(columns=['review', 'sentiment']).tail(9916)     # we use only the 300 embeddings\n",
    "y_test = df.tail(9916).sentiment\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('X_train shape check: ', X_train.shape)\n",
    "print('X_test shape check: ', X_test.shape)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "\n",
    "# GridSearch()\n",
    "###########################################\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "              'clf__n_estimators': [100, 300, 500, 1000],\n",
    "              'clf__learning_rate': [0.001, 0.01, 0.1, 1.0],\n",
    "              'clf__max_depth': [1, 10, 20]\n",
    "              }\n",
    "\n",
    "pipe = Pipeline([('clf', XGBClassifier())\n",
    "                ])\n",
    "                    \n",
    "# on cross-validation parameters\n",
    "cv = StratifiedKFold(n_splits=5, \n",
    "                     shuffle=False\n",
    "                     )\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(pipe, param_grid,\n",
    "                           scoring='roc_auc',\n",
    "                           cv=cv, \n",
    "                           n_jobs=)        # insert the number of jobs, according to the used machine\n",
    "\n",
    "# running the grid\n",
    "###########################################\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "# best estimator - test performance\n",
    "###########################################\n",
    "\n",
    "clf_b = gs_lr_tfidf.best_estimator_\n",
    "y_pred_proba = clf_b.predict_proba(X_test)\n",
    "y_pred = clf_b.predict(X_test)\n",
    "\n",
    "# AUC on test data\n",
    "auc_res=roc_auc_score(y_test, y_pred_proba[:, 1])\n",
    "\n",
    "# Accuracy on test data\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# collecting results\n",
    "###########################################\n",
    "\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('Test AUC: %.3f' % auc_res)\n",
    "print('Test Accuracy: %.3f' % acc)\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')\n",
    "print('---------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
