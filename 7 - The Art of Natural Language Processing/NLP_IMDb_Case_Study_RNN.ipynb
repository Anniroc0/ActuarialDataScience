{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *The Art of Natural Language Processing: RNNs for the Case Study*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Authors: Andrea Ferrario, Mara Nägelin**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Date: February 2020**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to run the RNNs in the Contemporary Approach, as described in the tutorial `The Art of Natural Language Processing: Classical, Modern and Contemporary Approaches to Text Document Classification'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Getting started with Python and Jupyter Notebook](#datagen)\n",
    "2. [Import data](#datagen)\n",
    "3. [Data preprocessing](#dataprep)  \n",
    "    3.1. [Remove duplicates](#remdup)  \n",
    "    3.2. [Shuffle the data](#shuffle)  \n",
    "    3.3. [Minimal preprocessing (detailed)](#prep_det)  \n",
    "    3.4. [Minimal preprocessing with Keras](#prep_keras)  \n",
    "4. [Deep learning](#DL)  \n",
    "    4.1. [Train test split](#trainsplit)  \n",
    "    4.2. [Define the model](#modeldef)  \n",
    "    .......4.2.1. [Shallow LSTM](#LSTM1)  \n",
    "    .......4.2.2. [Shallow GRU](#GRU1)  \n",
    "    .......4.2.3. [Deep LSTM](#LSTM2)    \n",
    "    4.3. [Train the model](#trainmodel)  \n",
    "    4.4. [Evaluate the model on test data](#evalmodel)  \n",
    "5. [Final remarks](#fm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"started\"></a>\n",
    "# 1. Getting started with Python and Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, Jupyter Notebook and Python settings are initialized. For code in Python, the [PEP8 standard](https://www.python.org/dev/peps/pep-0008/) (\"PEP = Python Enhancement Proposal\") is enforced with minor variations to improve readibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook settings\n",
    "###################\n",
    "\n",
    "# resetting variables\n",
    "get_ipython().magic('reset -sf') \n",
    "\n",
    "# formatting: cell width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='datagen'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the raw data from the original 50'000 text files and save to a dataframe. This only needs to be run once. After that, one can start directly with [Section 2](#dataprep). The following code snippet is based on the book `Python Machine Learning` by Raschka and Mirjalili, Chapter 8 (see tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "basepath = 'path_to_extracted_data/aclImdb/' # TODO: update to point to your data repository\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file), \n",
    "                      'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], \n",
    "                           ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv \n",
    "path = \"path_to_save_data/movie_data.csv\" # TODO: update to your path\n",
    "df.to_csv(path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dataprep'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we preprare the raw data such that in can be used as input for a neural network. Again, we follow the example of Raschka and Mirjalili (Chapter 16). We perform the following steps:\n",
    "\n",
    "* We remove all duplicates.\n",
    "* We shuffle the data in a random permutation.\n",
    "* We apply only minimal preprocessing (i.e. convert to lowercase and split on whitespaces and punctuation).\n",
    "* We map each word bijectively to an integer value.\n",
    "* We set each review to an equal length $T$ by padding with $0$ or slicing as required.\n",
    "\n",
    "The last three steps are written out in detail in [Section 2.3.](#prep_det) to give the reader an understanding of what exactly happens to the data. However, they can also be carried out — almost equivalently — using the high-level `text.preprocessing` functionalities of the `tensorflow.keras` module, see [Section 2.4.](#prep_keras) The user needs to run only one of these two subsections to preprocess the data.\n",
    "\n",
    "The transformed data is stored in a dataframe for convenience. Hence [Section 2](#dataprep) also needs to be run only once, and after one can start jump directly to [Section 3](#DL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following can be used to reimport the dataframe with the raw data generated in [Section 1](#datagen) above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "path = 'path_to_save_data/movie_data.csv' # TODO: update to your path  \n",
    "df = pd.read_csv(path, encoding='utf-8') # read in the dataframe stored as csv\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='remdup'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates - we found them, even with HTML markup...\n",
    "duplicates = df[df.duplicated()]  # Duplicated rows, except the first entry, are marked as 'True'\n",
    "print(len(duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a check on the duplicated review\n",
    "duplicates.review   # some appear more than once, as they originally appear 3 or more times in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates: 49582 + 418 = 50000\n",
    "df = df.drop_duplicates()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check\n",
    "df[df.duplicated(subset='review')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='shuffle'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We shuffle the data to ensure randomness in the training input\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df = df.reset_index(drop=True) # reset the index after the permutation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prep_det'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Minimal preprocessing (detailed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippets are in part adapted from Raschka and Mirjalili (Chapter 16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal preprocessing and generating word counts:\n",
    "#  - we surround all punctuation by whitespaces\n",
    "#  - all text is converted to lowercase\n",
    "#  - word counts are generated by splitting the text on whitespaces\n",
    "import pyprind\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "counts = Counter()\n",
    "pbar = pyprind.ProgBar(len(df['review']), title='Counting words occurrences')\n",
    "for i,review in enumerate(df['review']):\n",
    "    text = ''.join([c if c not in punctuation else ' '+c+' '  \n",
    "                    for c in review]).lower()\n",
    "    df.loc[i,'review'] = text\n",
    "    pbar.update()\n",
    "    counts.update(text.split()) # splitting on whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the size of the vocabulary\n",
    "print('Number of unique words:', len(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate how many words appear only rarely in the reviews\n",
    "print('Number of words that appear more than once:', \n",
    "      len([k for k, v in counts.items() if v > 1]))\n",
    "print('Number of words that appear more than 30 times:', \n",
    "      len([k for k, v in counts.items() if v > 30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hence we use only the 15'000 most common in our vocabulary \n",
    "# this will make training more efficient without loosing too much information\n",
    "vocab_size = 15000\n",
    "\n",
    "# create a dictionary with word:integer pairs for all unique words\n",
    "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
    "word_counts = word_counts[0:vocab_size]\n",
    "word_to_int = {word: ii for ii, word in enumerate(word_counts, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping words to integers\n",
    "# create a list with all reviews in integer coded form\n",
    "mapped_reviews = []\n",
    "pbar = pyprind.ProgBar(len(df['review']), title='Map reviews to ints')\n",
    "for review in df['review']:\n",
    "    mapped_reviews.append([word_to_int[word] \n",
    "                           for word in review.split() \n",
    "                           if word in word_to_int.keys()])\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the median length of the mapped review sequences to inform the choice of sequence_length\n",
    "print('Median length of mapped reviews:',\n",
    "      np.median([len(review) for review in mapped_reviews]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "rev_lengths = np.array([len(review) for review in mapped_reviews])\n",
    "plt.hist(rev_lengths[rev_lengths < 500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding: set sequence length and ensure all mapped reviews are coerced to required length\n",
    "# if sequence length < T: left-pad with zeros\n",
    "# if sequence length > T: use the last T elements\n",
    "sequence_length = 200  # (Known as T in our RNN formulae)\n",
    "sequences = np.zeros((len(mapped_reviews), sequence_length), dtype=int)\n",
    "\n",
    "for i, row in enumerate(mapped_reviews):\n",
    "    review_arr = np.array(row)\n",
    "    sequences[i, -len(row):] = review_arr[-sequence_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df with processed data\n",
    "df_processed = pd.concat([df['sentiment'],pd.DataFrame(sequences)], axis=1)\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='prep_keras'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Minimal preprocessing with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "vocab_size = 15000\n",
    "\n",
    "# map words to integers including minimal preprocessing\n",
    "tokenizer = Tokenizer(num_words=vocab_size, \n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', # filters out all punctuation other than '\n",
    "                      lower=True, # convert to lowercase\n",
    "                      split=' ') # split on whitespaces\n",
    "tokenizer.fit_on_texts(df['review'])\n",
    "list_tokenized = tokenizer.texts_to_sequences(df['review'])\n",
    "\n",
    "# Padding to sequence_length\n",
    "sequence_length = 200\n",
    "sequences = pad_sequences(list_tokenized, maxlen=sequence_length)\n",
    "\n",
    "# create df with processed data\n",
    "df_processed = pd.concat([df['sentiment'], pd.DataFrame(sequences)], axis=1)\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>  \n",
    "<br>\n",
    "Lastly, we save the fully preprocesssed data to csv for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv \n",
    "path = \"path_to_save_data/movie_data_processed.csv\" # TODO: update to your path\n",
    "df_processed.to_csv(path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='DL'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we reproduce the results from Section 6.4.6-6.4.7 of the tutorial. We split the preprocessed data into a training and a testing set and define our RNN model (three different possible architectures are given as examples, see [Sections 4.2.1.-4.2.3](#LSTM1). The model is compiled and trained using the high-level `tensorflow.keras` API. Finally, the development of loss and accuracy during the training is plotted and the fitted model is evaluated on the test data.  \n",
    "\n",
    "**WARNING:** Note that training with a large training dataset for a large number of epochs is computationally intensive and might easily take a couple of hours on a normal CPU machine. We recommend subsetting the training and testing datasets and/or using an HPC infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to ensure that all keras functionalities work as intended\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "path = \"path_to_save_data/movie_data_processed.csv\" # TODO: update to your path\n",
    "df_processed = pd.read_csv(path, encoding='utf-8')\n",
    "df_processed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='trainsplit'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of samples for the training and test datasets\n",
    "perc_train = 0.8\n",
    "n_train = round(df_processed.shape[0]*perc_train)\n",
    "n_test = round(df_processed.shape[0]*(1-perc_train))\n",
    "\n",
    "print(str(int(perc_train*100))+'/'+str(int(100-perc_train*100))+' train test split:', \n",
    "      n_train, '/', n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training and testing datasets\n",
    "X_train = np.array(df_processed.head(n_train).drop('sentiment', axis=1)) # replace with n_train\n",
    "y_train = df_processed.head(n_train).sentiment.values\n",
    "\n",
    "X_test = np.array(df_processed.tail(n_test).drop('sentiment', axis=1)) # replace with n_test\n",
    "y_test = df_processed.tail(n_test).sentiment.values\n",
    "\n",
    "print('Training data shape check X, y:', X_train.shape, y_train.shape)\n",
    "print('Testing data shape check X, y:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='modeldef'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# get the total number of words in vocabulary (+1 for the padding with 0)\n",
    "vocab_size = df_processed.drop('sentiment', axis=1).values.max() + 1\n",
    "print('Vocabulary size:', vocab_size-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the following subsections defines a distinct model architecture. The user can select and run one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='LSTM1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. Shallow LSTM architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This a shallow RNN with just one LSTM layer. The same architecture was used for the example by Raschka and Marjili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Add an Embedding layer expecting input of the size of the vocabulary, and\n",
    "# the embedding output dimension\n",
    "model.add(layers.Embedding(input_dim=vocab_size, output_dim=256)) \n",
    "\n",
    "# Add a LSTM layer with 128 internal units\n",
    "model.add(layers.LSTM(128))\n",
    "\n",
    "# Add a Dropout layer to avoid overfitting\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Add Dense layer as output layer with 1 unit and sigmoid activation\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='GRU1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Shallow GRU architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is essentially the same shallow RNN as above with a GRU layer instead of the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Add an Embedding layer expecting input of the size of the vocabulary, and\n",
    "# the embedding output dimension\n",
    "model.add(layers.Embedding(input_dim=vocab_size, output_dim=256)) \n",
    "\n",
    "# Add a GRU layer with 128 internal units\n",
    "model.add(layers.GRU(128))\n",
    "\n",
    "# Add a Dropout layer to avoid overfitting\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Add Dense layer as output layer with 1 unit and sigmoid activation\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='LSTM3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3. Deep LSTM architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily deepen our network by stacking a second LSTM layer on top of the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Add an Embedding layer expecting input of the size of the vocabulary, and\n",
    "# the embedding output dimension\n",
    "model.add(layers.Embedding(input_dim=vocab_size, output_dim=256)) \n",
    "\n",
    "# Add a LSTM layer with 128 internal units\n",
    "# Return sequences so we can stack the the next LSTM layer on top\n",
    "model.add(layers.LSTM(128, return_sequences=True))\n",
    "\n",
    "# Add a second LSTM layer\n",
    "model.add(layers.LSTM(128))\n",
    "\n",
    "# Add a Dropout layer to avoid overfitting\n",
    "model.add(layers.Dropout(0.5))\n",
    "\n",
    "# Add Dense layer as output layer with 1 unit and sigmoid activation\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='trainmodel'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the summary of the model we have defined\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "# we select here the optimizer, loss and metric to be used in training\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callbacks for early stopping during training\n",
    "# stop training when the validation loss `val_accuracy` is no longer improving\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    min_delta=1e-3,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "history = model.fit(X_train, \n",
    "                    y_train, \n",
    "                    validation_split=0.2,\n",
    "                    epochs=30, \n",
    "                    batch_size=256,\n",
    "                    callbacks=callbacks,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the development of the accuracy over epochs to see the training progress\n",
    "plt.plot(history.history['accuracy'], label='Training accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the development of the loss over epochs to see the training progress\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='evalmodel'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Evaluate the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# evalute model to get accuracy and loss on test data\n",
    "results = model.evaluate(X_test, y_test, batch_size=256, verbose=0)\n",
    "\n",
    "# calculate AUC on test data\n",
    "y_pred = model.predict_proba(X_test, batch_size=256)\n",
    "auc_res = roc_auc_score(y_test, y_pred[:, 0])\n",
    "\n",
    "print('Test loss:', results[0])\n",
    "print('Test accuracy:', results[1])\n",
    "print('Test AUC:', auc_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fm'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Final remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example RNNs are simple architectures where none of the parameters were optimized for performance. In order to further improve the model accuracy, we could for example\n",
    "* play around with the network architecture    \n",
    "     (e.g. the depth of the network, the type of layers used, the number of hidden units within a layer, the activation functions used, ...)\n",
    "* fine-tune the training parameters   \n",
    "     (i.e. the number of epochs, batch size, ...)\n",
    "* perform more elaborate preprocessing on the data  \n",
    "    (e.g. excluding stopwords, see also the two other Notebooks and Section 1 of the tutorial)\n",
    "* use a the weights of an already trained embedding for our embedding layer  \n",
    "    (either as nontrainable fixed weights or with transfer learning, compare the Notebook ```NLP_IMDb_Case_Study_ML.ipynb``` and Section 3 of the tutorial)  \n",
    "\n",
    "Finally, note that the size of the dataset is arguably still too small to allow for much improvement over the presented architectures and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP)",
   "language": "python",
   "name": "nlp_chapter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
