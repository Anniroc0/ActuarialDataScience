{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *The Art of Natural Language Processing: NLP Pipeline*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Authors: Andrea Ferrario, Mara Nägelin**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Date: February 2020**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to test NLP preprocessing pipelines, as described in the tutorial `The Art of Natural Language Processing: Classical, Modern and Contemporary Approaches to Text Document Classification'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Getting started with Python and Jupyter Notebook](#started)\n",
    "2. [Test sentence](#test)\n",
    "3. [NLP preprocessing pipelines](#pipeline)  \n",
    "    3.1. [Conversion of text to lowercase](#lower)  \n",
    "    3.2. [Tokenizers](#tokenizers)  \n",
    "    3.3. [Stopwords removal](#stopwords)  \n",
    "    3.4. [Part-of-speech tagging](#POS)  \n",
    "    3.5. [Stemming and lemmatization](#stemming)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Getting started with Python and Jupyter Notebook<a name=\"started\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, Jupyter Notebook and Python settings are initialized. For code in Python, the [PEP8 standard](https://www.python.org/dev/peps/pep-0008/) (\"PEP = Python Enhancement Proposal\") is enforced with minor variations to improve readibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook settings\n",
    "###################\n",
    "\n",
    "# resetting variables\n",
    "get_ipython().magic('reset -sf') \n",
    "\n",
    "# formatting: cell width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# plotting\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Test sentence<a name=\"test\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce the test sentence to be preprocessed with NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"In H.P. Lovecraft's short story 'The Call of Cthulhu', the author states that in S. Latitude 47° 9', W. Longitude 126° 43' the great Cthulhu dreams in the sea-bottom city of R'lyeh.\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the NLP pipeline:\n",
    "- conversion of text to lowercase;\n",
    "- tokenization, i.e. split of all strings of text into tokens;\n",
    "- part-of-speech (POS) tagging of tokenized text;\n",
    "- stopwords removal;\n",
    "- stemming or lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. NLP Preprocessing Pipeline<a name=\"pipeline\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Conversion of text to lowercase<a name=\"lower\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply lowercase to the test sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Tokenizers<a name=\"tokenizers\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. whitespace tokenizer\n",
    "#########################\n",
    "import re\n",
    "white_tok = text.split()\n",
    "print(white_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Natural Language Tool Kit tokenizer\n",
    "########################################\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens_NLTK = word_tokenize(text)\n",
    "print(tokens_NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. SpaCy tokenizer\n",
    "####################\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. sklearn vectorizer\n",
    "#######################\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "textt = [text]\n",
    "X = vectorizer.fit_transform(textt)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Stopwords removal<a name=\"stopwords\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now remove stopwords using NLTK, SpaCy and sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. stopwords removal with the Natural Language Tool Kit (NLTK)\n",
    "################################################################\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# we tokenize the test sentence\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "filtered_tokens = [word for word in tokens if word not in stop]\n",
    "\n",
    "print('-----------------------------')\n",
    "print('NLTK tokenized test sentence:', tokens)\n",
    "print('-----------------------------')\n",
    "print('NLTK tokenized test sentence after stowords removal:', filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed stopwords\n",
    "list(set(tokens) - set(filtered_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. stopwords removal with SpaCy\n",
    "#################################\n",
    "import spacy\n",
    "\n",
    "# tokenization\n",
    "text_spacy = nlp(text)\n",
    "\n",
    "token_list = []\n",
    "for token in text_spacy:\n",
    "    token_list.append(token.text)\n",
    "\n",
    "# stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# create list of word tokens after removing stopwords note that .vocab() looks at the lexeme of each token \n",
    "filtered_sentence =[] \n",
    "\n",
    "for word in token_list:\n",
    "    lexeme = nlp.vocab[word]   \n",
    "    if lexeme.is_stop == False:\n",
    "        filtered_sentence.append(word) \n",
    "        \n",
    "print('-----------------------------')\n",
    "print('SpaCy tokenized test sentence:', token_list)\n",
    "print('-----------------------------')\n",
    "print('SpaCy tokenized test sentence after stowords removal:', filtered_sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed stopwords\n",
    "list(set(token_list) - set(filtered_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. stopwords removal with sklearn and TfidfVectorizer()\n",
    "########################################################\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [text]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer_stop = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X_stop = vectorizer_stop.fit_transform(corpus)\n",
    "\n",
    "print('-----------------------------')\n",
    "print('CountVectorizer() tokenized test sentence:', vectorizer.get_feature_names())\n",
    "print('-----------------------------')\n",
    "print('CountVectorizer() tokenized test sentence after stowords removal:', vectorizer_stop.get_feature_names()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed stopwords\n",
    "set(vectorizer.get_feature_names()) - set(vectorizer_stop.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Part-of-speech tagging<a name=\"POS\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform Part-Of-Speech (POS) tagging using the NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# introduction of POS tagger per NLTK token (word_tokenize is the tokenizer we choose)\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "tokens_NLTK = word_tokenize(text)\n",
    "print(pos_tag(word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Stemming and lemmatization<a name=\"stemming\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform (Porter) stemming and lemmatization on the test sentence, after tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Porter stemming on tokenized test sentence \n",
    "#################################################\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in word_tokenize(text)]\n",
    "\n",
    "# use stemming on NLTK tokenizer text\n",
    "stem_tokens = tokenizer_porter(text)\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK lemmatization (WordNet database) on tokenized test sentence\n",
    "##################################################################\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "# Wordnet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# NLTK tokenizer\n",
    "word_list = nltk.word_tokenize(text)\n",
    "\n",
    "# lemmatization of the list of words and join - we lemmatize verbs (therefore 'v') and we use '***' as separator\n",
    "lemmatized_output = '***'.join([lemmatizer.lemmatize(w, 'v') for w in word_list])\n",
    "print(lemmatized_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
